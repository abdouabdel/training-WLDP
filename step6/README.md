# Using Openshift Volume

In this step, we will improve our `hello` and `names` services to be able to persist and share files between them.

The goal will be to write in a file each name generated by the service `names` then share this file that service `hello` will read to display in his response all people that have been greeted. This will give us an opportunity to see how persistence and sharing data works for containers.

As we know, Openshift Origin is based on Docker and Kubernetes. By default, On-disk files in a container are ephemeral which cause somes problems. firstly, when container restarts it will be in clean state and files are lost, secondly, when running containers together in a Pod it is often necessary to share files between those containers.

In order to be able to persist data and also to share data between containers, Docker came up with the concept of volumes but less managed and limited in providing volumes plugins than the Kubernetes Volume abstraction.

<https://kubernetes.io/docs/concepts/storage/persistent-volumes/>
<https://kubernetes.io/docs/concepts/storage/volumes/>

Openshift uses the Kubernetes volume abstraction which involves knowing notions such as `emptyDir`, `PersistentVolume`, `PersistentVolumeClaim`, `storageClass`, `dynamic` and `static` provisionning:

* `emptyDir` is the simplest volume type which is a temporary empty directory on a single machine used to write ephemeral data by the application. It is created when a pod is assigned to a node, and exists as long as that pod is running on that node (For each pod created an emptyDir is created and the data between them are not shared even if on the same node)

  EmptyDir volume is automatically created when deploying an app that specify using Docker volumes (using VOLUME instruction in dockerfile).

  It does not allow sharing data between pods but can be helpful when saving temporary data.

* A `PersistentVolume` (`pv`) is a piece of storage in the cluster that has been provisioned.

* A `PersistentVolumeClaim` (`pvc`) is a request for storage by a user. It consumes available PV resources created earlier.

   Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only).

* `Dynamic` volume provisioning allows storage volumes to be created automatically on-demand.

* `Static` volume provisioning implies that cluster administrators have to manually make calls to their cloud or storage provider to create new storage volumes, and then create PersistentVolume objects.

* `storageClass` object specify the provisioner to use to provision a volume and parameters to pass to that provisioner when provisioning.

The dynamic provisioning feature eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage when a `pvc` is requested with a storageClass specifying to use dynamic provisionnig.

We will use dynamic provisioning since it does not require admin roles.

Before that lets modify the services `names` and `hello` from step2.

## Improving hello and names services

As said above, the `names` service have to write each generated name in a file and the `hello` service have to access this file, read it and display all the greeted people.

### Service `names`

`names` service has been modified from step2 to step6 to write in `names.txt` file each names generated by the service. `names.txt` is located under the directory named `files`.

```python
class NamesServerRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

  # the path of names.txt
  FILE_PATH = "../files/names.txt"

  def do_GET(self):
    self.protocol_version = 'HTTP/1.1'

    status = 200

    if (self.path == '/'):
      response = names.get_full_name()
    elif (self.path == '/male'):
      response = names.get_full_name(gender='male')
    elif (self.path == '/female'):
      response = names.get_full_name(gender='female')
    else:
      status = 404
      response = "Not Found"

    # if the request success we try to open names.txt file to write the response and then close file.
    if (status != 404) :
      try:
        f = open(NamesServerRequestHandler.FILE_PATH,"a")
        f.writelines(response+"\n")
      except Exception as e:
        print "[error] Failed to write data: ", e
        f.close()
      else:
        print "[success] Data written: ", response
        f.close()
```

> This example may have limitations if multiple instances of the `names` service try to write on the same file.  By doing this way, we trying to focus on the use of volume.

To test the modification, you have to create the directory `files` under `names`:

* in local machine

```bash
$ mkdir names/files
.
├── hello
│   ├── Dockerfile
│   ├── run.sh
│   └── src
│       └── app.py
├── names
│   ├── Dockerfile
│   ├── files
│   ├── run.sh
│   └── src
│       └── app.py
└── README.md
```

* for docker image, you have also to give write access on it:

```yaml
...

ADD src/app.py run.sh ${HOME}/

RUN mkdir /opt/app-root/files # create a the files directory inside the image

RUN chgrp -R 0 /opt/app-root/files && chmod -R g+rwX /opt/app-root/files # give write permission to the application

VOLUME ["/opt/app-root/files"] # expose files drectory as a volume

WORKDIR ${HOME}

...
```

### Service `hello`

`hello` service has been modified from step2 to step6 to read `names.txt` file and then to display everything in it. The path of `names.txt` is where it will be mount: in our case `/opt/app-root/files/names.txt`.

```python
class HelloServerRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

  # way in python to disable proxy auto-detection to make direct connections 
  NOPROXY_OPENER = urllib2.build_opener(urllib2.ProxyHandler({})) 

  # way in python to rely on system proxy config (env var, ...) 
  SYSTEMPROXY_OPENER = urllib2.build_opener(urllib2.ProxyHandler()) 

  # the path of names.txt
  FILE_PATH = "/opt/app-root/files/names.txt"

  ...

  def do_GET(self):
    self.protocol_version = 'HTTP/1.1'

    status = 404
    response = "Not Found"

    if (self.path == '/'):
      status, name = self.get_url('http://names:8080/')

      # read names.txt file to display its contains.
      try:
        lines = open(HelloServerRequestHandler.FILE_PATH,"r").readlines()
      except Exception as e:
        print "[error] Failed to read data: ", e
        response = 'Hello ' + name + '!\n' 
      else:
        response = "Hello " + name + "!\n\n"+\
        "------------------------\n"+\
        "People recently greeted: \n"+\
        "------------------------\n\n"+\
        '\n'.join(map(str, lines[::-1]))
```

After this modification, you can deploy to Openshift both `names` and `hello` services

## Use a PersistentVolume for Storage

> We assume that you are still logged with credentials given at step1 and you are using project "training-test"

To use persistent volume storage follow the process :

1. create a PersistentVolumeClaim using a dynamic provisioner, which will be bound to a suitable PersistentVolume created automatically.

1. mount on both `names` and `hello` services the newly PersistentVolumeClaim as storage.

### Create a PersistentVolumeClaim

We use PersistentVolumeClaims to request physical storage.

Yaml below represents the PersistentVolumeClaim named training-test-claim that requests a volume of 1 gibibytes that can provide read-write access. It is annotated to use the storageClass `dynamic` which use a dynamic provisioner to create suitable `PersistentVolume`.

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: training-test-claim
  annotations:
    volume.beta.kubernetes.io/storage-class: dynamic
spec:
  storageClassName: dynamic
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

Create a file, `pvc.yml` with configuration above as its content or use the one provided in this step. Then run the command `oc create -f <PATH_TO_YAML_FILE>` to create our `pvc`: `training-test-claim`.

```bash
$ oc create -f pvc.yml
persistentvolumeclaim "training-test-claim" created
```

Then run `oc get pvc/training-test-claim` to see the status of the pvc and the `PersistentVolume` bound to it.

```bash
$  oc get pvc/training-test-claim
NAME                  STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
training-test-claim   Bound     pvc-e7eaa104-793e-11e7-804e-005056955e11   1Gi        RWO           5m
```

> More info about storage: <https://docs.openshift.com/container-platform/3.5/architecture/additional_concepts/storage.html>

### Mount the storage

Now as the storage is available, lets mount it on the applications.

To do so, we will run this command `oc volume dc/<name> --add --name=<volumeName> --type=persistentVolumeClaim --claim-name=<pvcName> --mount-path=<pathToMount>`.

This command will add a volume on the specified object, a deploymentConfig here; options will permit to set a name to the volume, to specify `pvc` as type of volume, the name of the `pvc` used and finally the path to mount.

> More info here: <https://docs.openshift.com/container-platform/3.5/dev_guide/volumes.html>

Run `oc get dc` to identify `deploymentConfig` where volume will be mounted on:

```bash
$  oc get dc
NAME      REVISION   DESIRED   CURRENT   TRIGGERED BY
hello     6          1         1         config,image(hello:latest)
names     9          1         1         config,image(names:latest)
```

Now add volume to `hello` and `names` using command above:

```bash
$  oc volume dc/names --add --name=v1 --type=persistentVolumeClaim --claim-name=training-test-claim --mount-path=/opt/app-root/files
deploymentconfig "names" updated
$ oc volume dc/hello --add --name=v1 --type=persistentVolumeClaim --claim-name=training-test-claim --mount-path=/opt/app-root/files
deploymentconfig "hello" updated
```

You can verify the configuration of each `deploymentConfig` for `names` and `hello`, to see the volume mounted:

```bash
$ oc get dc/names -o yaml
apiVersion: v1
kind: DeploymentConfig
metadata:
  labels:
    app: names
  name: names
  namespace: training-test
  ...
spec:
  replicas: 1
  ...
  template:
    metadata:
      annotations:
        openshift.io/generated-by: OpenShiftWebConsole
      creationTimestamp: null
      labels:
        app: names
        deploymentconfig: names
    spec:
      containers:
      - image: 172.30.8.127:5000/training-test/names@sha256:3eb86071b658b3499dc6244d16ded2ec8789fc50b9160349625991faa36256e7
        imagePullPolicy: Always
        name: names
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        volumeMounts:
        - mountPath: /opt/app-root/files/
          name: volume-zzp2c
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
      - name: volume-zzp2c
        persistentVolumeClaim:
          claimName: training-test-claim

```

```bash
$ oc get dc/hello -o yaml
...
spec:
      containers:
      - image: 172.30.8.127:5000/training-elji/hello@sha256:dc1bf0665865cf64f6ded5dc53a41cf606b1739f2098588ecd3cdba7222dc483
        imagePullPolicy: Always
        name: hello
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        volumeMounts:
        - mountPath: /opt/app-root/files/
          name: volume-7dl7h
          readOnly: true
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
      - name: volume-7dl7h
        persistentVolumeClaim:
          claimName: training-test-claim
...
```

Finally, request the `hello` service to see the output:

```bash
$ curl http://hello-training-test.wldp.fr/
Hello Tiffany Stone!

------------------------
People recently greeted:
------------------------

Tiffany Stone

Ian Garner

Eugenia Johnson

Danielle Elizondo

Linda Obhof

Salvador Roberts

Jesus Gannon

Cira Buttner

Marjorie Snell

```

## EmptyDir limitations

In this section, we will simply demonstrate the limitations, as said earlier, when using `emptyDir` as the volume type.

To use an `emptyDir` volume we will run the `oc volume` command specifying the type of the volume as `emptyDir`.

Let's use it for the `names` and `hello` apps:

We remove the volume and the pvc created earlier:

```bash
# removing the volume mount on names
$ oc volume dc/names --remove --name=v1
deploymentconfig "names" updated
# removing the volume on hello
$ oc volume dc/hello --remove --name=v1
deploymentconfig "hello" updated
# removing the pvc
$ oc delete -f pvc.yml
```

We mount emptyDir volumes on each names and hello services

```bash
$ oc volume dc/names --add --name=v2 --type=emptyDir --mount-path=/opt/app-root/files
deploymentconfig "names" updated
$ oc volume dc/hello --add --name=v2 --type=emptyDir --mount-path=/opt/app-root/files
deploymentconfig "hello" updated
```

When we request hello, we will see that:

* in `names` pod the name generated will be written on `names.txt` file but, the list of greeted people will not appear on `hello` service response because there a no `names.txt` file found: the `names.txt` of `names` emptyDir volume is not shared.

  ```bash
  # get running pods
  $ oc get pods
  NAME            READY     STATUS    RESTARTS   AGE
  hello-3-gw0jb   1/1       Running   0          17s
  names-3-j8zws   1/1       Running   0          36s
  # requesting hello service
  $ curl  http://hello-training-elji.wldp.fr
  Hello Christopher Dickerson!
  # as we see the list of greeted people is not present because names.txt is not found as described in the logs of hello service below:
  $ oc logs pod/hello-3-gw0jb
  starting hello server (pid=1)...
  listening on 0.0.0.0:8080
  [error] Failed to read data:  [Errno 2] No such file or directory: '/opt/app-root/files/names.txt'
  10.130.4.1 - - [23/Aug/2017 07:42:18] "GET / HTTP/1.1" 200 -
  # openig a remote shell session of names container
  $ oc rsh names-3-j8zws
  # displaying the content of the names.txt file in names container
  sh-4.2$ cat ../files/names.txt
  Christopher Dickerson
  ```

* in `names` pod the name generated will be written on `names.txt` file but if the pod is not running anymore, all content in `names.txt` will be lost.

  ```bash
  # to shutdown the pod, we scale down the names service to 0
  $ oc scale dc/names --replicas=0
  deploymentconfig "names" scaled
  # then we start a new one by scaling up to 1
  $ oc scale dc/names --replicas=1
  deploymentconfig "names" scaled
  # we get the name of the new pod created
  $ oc get pods
  NAME            READY     STATUS    RESTARTS   AGE
  hello-3-gw0jb   1/1       Running   0          25m
  names-3-239df   1/1       Running   0          2m
  # open a remote shell session on it
  $ oc rsh names-3-239df
  # here we can see that the names.txt created earlier is gone
  sh-4.2$ ls -l ../files/
  total 0
  ```